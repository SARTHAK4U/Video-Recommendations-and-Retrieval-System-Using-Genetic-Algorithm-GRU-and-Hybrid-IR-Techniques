{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installations And Pretrained Models File Fetch**","metadata":{}},{"cell_type":"markdown","source":"Installing libraries and important files","metadata":{}},{"cell_type":"code","source":"!pip install -U flask-cors\n!pip install flask-ngrok\n!pip install pyngrok\n!pip install -U sentence_transformers\n!pip install keras-preprocessing\n!pip install sk-video\n!wget -O 'c3d.py' 'https://drive.google.com/uc?id=1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j&confirm=t'\n!wget -O 'sports1M_utils.py' 'https://drive.google.com/uc?id=1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i&confirm=t'","metadata":{"execution":{"iopub.status.busy":"2023-04-01T20:43:41.391300Z","iopub.execute_input":"2023-04-01T20:43:41.391749Z","iopub.status.idle":"2023-04-01T20:45:03.729734Z","shell.execute_reply.started":"2023-04-01T20:43:41.391707Z","shell.execute_reply":"2023-04-01T20:45:03.728465Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting flask-cors\n  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\nRequirement already satisfied: Six in /opt/conda/lib/python3.7/site-packages (from flask-cors) (1.16.0)\nRequirement already satisfied: Flask>=0.9 in /opt/conda/lib/python3.7/site-packages (from flask-cors) (2.2.3)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.9->flask-cors) (4.11.4)\nRequirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.9->flask-cors) (8.1.3)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.9->flask-cors) (3.1.2)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.9->flask-cors) (2.2.3)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.9->flask-cors) (2.1.2)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.9->flask-cors) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.9->flask-cors) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.1)\nInstalling collected packages: flask-cors\nSuccessfully installed flask-cors-3.0.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting flask-ngrok\n  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\nRequirement already satisfied: Flask>=0.8 in /opt/conda/lib/python3.7/site-packages (from flask-ngrok) (2.2.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from flask-ngrok) (2.28.2)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.8->flask-ngrok) (2.2.3)\nRequirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.8->flask-ngrok) (4.11.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->flask-ngrok) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->flask-ngrok) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->flask-ngrok) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->flask-ngrok) (1.26.14)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.8->flask-ngrok) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.8->flask-ngrok) (4.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.1)\nInstalling collected packages: flask-ngrok\nSuccessfully installed flask-ngrok-0.0.25\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pyngrok\n  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from pyngrok) (6.0)\nBuilding wheels for collected packages: pyngrok\n  Building wheel for pyngrok (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=93f0ebc522be6cdc77e9d03add8def09b411a29a6674cc83ac0961acd94c674b\n  Stored in directory: /root/.cache/pip/wheels/b8/5b/f0/7e06eee2630e196f6ad1d84319e4ef1b4512212beb9562ac24\nSuccessfully built pyngrok\nInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-5.2.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting sentence_transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (4.26.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.13.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.14.0+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.21.6)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.0.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (1.7.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.1.97)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from sentence_transformers) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.11.4)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.11.10)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk->sentence_transformers) (1.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence_transformers) (9.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.11.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.14)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\nBuilding wheels for collected packages: sentence_transformers\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=754914a9c7a7590558185bc2bd86c881b4117813919f35c455f97bd7b6e6100d\n  Stored in directory: /root/.cache/pip/wheels/83/71/2b/40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\nSuccessfully built sentence_transformers\nInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting keras-preprocessing\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-preprocessing) (1.21.6)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras-preprocessing) (1.16.0)\nInstalling collected packages: keras-preprocessing\nSuccessfully installed keras-preprocessing-1.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting sk-video\n  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from sk-video) (1.7.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from sk-video) (1.21.6)\nInstalling collected packages: sk-video\nSuccessfully installed sk-video-1.1.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m--2023-04-01 20:44:59--  https://drive.google.com/uc?id=1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j&confirm=t\nResolving drive.google.com (drive.google.com)... 64.233.188.102, 64.233.188.101, 64.233.188.139, ...\nConnecting to drive.google.com (drive.google.com)|64.233.188.102|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://doc-0s-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b1rnghmo7rpvo6lcq3nipru5h4cpj029/1680381900000/17228267550831568869/*/1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j?uuid=469cf595-22d6-4b9f-92b6-ffd951a0c572 [following]\nWarning: wildcards not supported in HTTP.\n--2023-04-01 20:45:00--  https://doc-0s-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b1rnghmo7rpvo6lcq3nipru5h4cpj029/1680381900000/17228267550831568869/*/1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j?uuid=469cf595-22d6-4b9f-92b6-ffd951a0c572\nResolving doc-0s-20-docs.googleusercontent.com (doc-0s-20-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\nConnecting to doc-0s-20-docs.googleusercontent.com (doc-0s-20-docs.googleusercontent.com)|64.233.189.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3177 (3.1K) [text/x-python]\nSaving to: ‘c3d.py’\n\nc3d.py              100%[===================>]   3.10K  --.-KB/s    in 0s      \n\n2023-04-01 20:45:00 (88.0 MB/s) - ‘c3d.py’ saved [3177/3177]\n\n--2023-04-01 20:45:01--  https://drive.google.com/uc?id=1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i&confirm=t\nResolving drive.google.com (drive.google.com)... 64.233.188.139, 64.233.188.138, 64.233.188.101, ...\nConnecting to drive.google.com (drive.google.com)|64.233.188.139|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://doc-0g-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rvfd8n0peov4u11pnt9e76schrtjklcv/1680381900000/17228267550831568869/*/1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i?uuid=ebaf20e9-514b-4f14-abe1-d62b4150992b [following]\nWarning: wildcards not supported in HTTP.\n--2023-04-01 20:45:03--  https://doc-0g-20-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rvfd8n0peov4u11pnt9e76schrtjklcv/1680381900000/17228267550831568869/*/1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i?uuid=ebaf20e9-514b-4f14-abe1-d62b4150992b\nResolving doc-0g-20-docs.googleusercontent.com (doc-0g-20-docs.googleusercontent.com)... 64.233.189.132, 2404:6800:4008:c07::84\nConnecting to doc-0g-20-docs.googleusercontent.com (doc-0g-20-docs.googleusercontent.com)|64.233.189.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2318 (2.3K) [text/x-python]\nSaving to: ‘sports1M_utils.py’\n\nsports1M_utils.py   100%[===================>]   2.26K  --.-KB/s    in 0s      \n\n2023-04-01 20:45:03 (68.6 MB/s) - ‘sports1M_utils.py’ saved [2318/2318]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Recommendations And Search Cell**","metadata":{}},{"cell_type":"code","source":"#recommender cell\nfrom flask import Flask\nfrom flask_ngrok import run_with_ngrok\nimport numpy as np\nfrom collections import defaultdict\nimport tqdm.notebook as tqdm #for loader graphic\nimport networkx as nx\n# import torch\nfrom sentence_transformers import SentenceTransformer, util\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport os\nimport re\nimport collections\nimport nltk\nimport numpy as mp\nimport pandas as pd\nfrom flask import request\nfrom flask import jsonify\ndf = pd.read_csv('/kaggle/input/datacaptions/data.csv')\ndf = df.groupby('VideoID', as_index=False).first()\n\n\n# len(df)\n\n\n#data cleaning of the descriptions\nnltk.download('punkt')\n# import requests\n# import bs4\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\n\ndf.head()\n\n\n#creating instance of sentence transformer\nmodel_transformer = SentenceTransformer('paraphrase-MiniLM-L6-v2')\nX = model_transformer.encode(df.Description.values)\n\ndef generate_results(test_sentence, status=1):\n    print('QUERIED DESC : ',test_sentence)\n    data = {}\n    Ids = []\n    \n    #encoding query vector\n    query_vec = model_transformer.encode(test_sentence)\n\n    #finding cosine similarity with vectors of database\n    results = sklearn.metrics.pairwise.cosine_similarity(\n        X, [query_vec], dense_output=True)\n\n    res = []\n    for i in range(len(results)):\n        res.append([results[i], i])\n   #sorting \n    res.sort(reverse=True)\n    if status == 0:\n        return res\n    # print('Recommended Video--------Recommendation')\n    \n    #returning top30 information retrieval results\n    for i in res[:20]:\n        data[df.iloc[i[1], 0]] = df.iloc[i[1], 1]\n        Ids.append(df.iloc[i[1], 0])\n        # print(df.iloc[i[1], 0], \"--------\", df.iloc[i[1], 1])\n    return data,Ids\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:26:18.491594Z","iopub.execute_input":"2023-04-01T21:26:18.492061Z","iopub.status.idle":"2023-04-01T21:26:24.917710Z","shell.execute_reply.started":"2023-04-01T21:26:18.492010Z","shell.execute_reply":"2023-04-01T21:26:24.916567Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/62 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4e0e386f58b40908499d26498cf07a2"}},"metadata":{}}]},{"cell_type":"markdown","source":"# **Video Description Generator**","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\nfrom tensorflow import keras\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras_preprocessing.text import Tokenizer\nfrom tensorflow.keras import backend as K1\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.applications.inception_v3 import InceptionV3\nimport os\nimport cv2\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport skvideo.io\nimport importlib as imp\nfrom c3d import C3D\nfrom keras.models import Model\nimport sports1M_utils\n\n\"\"\"# ***Video PreProcessing***\"\"\"\n\nK = 28\n\n\n#function to fetch equally spaced k frames\ndef extract_frames_equally_spaced(frames, K):\n    n_frames = len(frames)\n\n    splits = np.array_split(range(n_frames), K)\n    idx_taken = [s[0] for s in splits]\n    sub_frames = []\n\n    for idx in idx_taken:\n        sub_frames.append(frames[idx])\n    return sub_frames\n\n#padding frames to make net frames same for all\ndef pad_frames(frames, limit, jpegs=False):\n    last_frame = frames[-1]\n    if jpegs:\n        frames_padded = frames + [last_frame]*(limit-len(frames))\n    else:\n        padding = np.asarray([last_frame * 0.]*(limit-len(frames)))\n        frames_padded = np.concatenate([frames, padding], axis=0)\n    return frames_padded\n\n#resizing of the frames\ndef resize_frames(frames):\n    new_frames = []\n    for frame in frames:\n        new_frame = cv2.resize(frame, (224, 224))\n        new_frames.append(new_frame)\n\n    return new_frames\n\n#extracting k frames from the videos\ndef video_to_frames(input_loc, K):\n    cap = cv2.VideoCapture(input_loc)\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n    count = 0\n\n    frames = []\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        frames.append(frame)\n        count = count + 1\n\n        if (count > (video_length-1)):\n            cap.release()\n            break\n    if len(frames) < K:\n        frames = pad_frames(frames, K, True)\n    else:\n        frames = extract_frames_equally_spaced(frames, K)\n\n    return frames\n\n\n#model for global feature extraction\ndef get_model():\n    i = tf.keras.layers.Input([224, 224, 3], dtype=tf.float32)\n    x = tf.keras.applications.inception_v3.preprocess_input(i)\n    model = InceptionV3(weights=\"imagenet\", include_top=False,\n                        input_shape=(224, 224, 3), pooling='avg')\n    out = model(x)\n    model = tf.keras.models.Model(inputs=[i], outputs=out)\n    return model\n    # model.summary()\n\n\n\ngoogle_net_model = get_model()\n\n#extracting global-features from the frames\ndef extract_global_features(save, path):\n    if not os.path.exists('global_features'):\n        os.makedirs('global_features')\n\n    frames = video_to_frames(path, K)\n    frames = resize_frames(frames)\n    frames = np.array(frames)\n\n    feature_vector = google_net_model.predict(frames, batch_size=128)\n    feature_path = 'global_features/'+'global'+'.npy'\n    np.save(feature_path, feature_vector)\n    return feature_vector\n\n\n\n#extracting motion features from the frames\ndef extract_frames_equally_spaced_motion(frames, K):\n    n_frames = len(frames)\n    if (n_frames < K):\n        while len(frames) < K:\n            frames.append(frames[-1])\n        return frames\n\n    # print('Nframes : ',n_frames)\n    splits = np.array_split(range(n_frames), K)\n    idx_taken = [s[0] for s in splits]\n    sub_frames = []\n\n    for idx in idx_taken:\n        sub_frames.append(frames[idx])\n    return sub_frames\n\n\n#creating instance of pretrained model\nbase_model = C3D(weights='sports1M')\nmodel_motion = Model(inputs=base_model.input,\n                  outputs=base_model.get_layer('fc6').output)\n\n\n# extracting motion features over windows using previously defined functions\ndef extract_motion_features(save, path):\n    imp.reload(sports1M_utils)\n    if not os.path.exists('motion_features1'):\n        os.makedirs('motion_features1')\n\n\n    \n    frames = video_to_frames(path, K)\n    frames = resize_frames(frames)\n    frames = np.array(frames)\n\n    windows = []\n    org_vid = frames\n    vid = []\n    motion_features_video = []\n    for i in range(len(frames)):\n        vid.append(frames[i])\n        if i == 15:\n            windows.append(vid)\n        elif i >= 16:\n            vid.pop(0)\n            windows.append(vid)\n    print(len(windows))\n    print(len(windows[0]))\n\n    windows = extract_frames_equally_spaced_motion(windows, 28)\n    for window in windows:\n        x = sports1M_utils.preprocess_input(np.array(window))\n        feature_vector = model_motion.predict(x)\n        feature_vector = feature_vector[0]\n        feature_vector = np.array(feature_vector)\n\n        motion_features_video.append(feature_vector)\n    motion_features_video = np.array(motion_features_video)\n\n    path = 'motion_features1/'+'motion'+'.npy'\n    np.save(path, motion_features_video)\n    return motion_features_video\n\n\n\n#attention mechanism class used to create model skeltal and load the weights obtained from the training of model\nclass attention(tf.keras.layers.Layer):\n\n    def __init__(self, return_sequences=True, **kwargs):\n        self.return_sequences = return_sequences\n        super(attention, self).__init__()\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'return_sequences': self.return_sequences\n        })\n        return config\n\n    def build(self, input_shape):\n\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(attention, self).build(input_shape)\n\n    def call(self, x):\n\n        e = K1.tanh(K1.dot(x, self.W)+self.b)\n        a = K1.softmax(e, axis=1)\n        output = x*a\n\n        if self.return_sequences:\n            return output\n\n        return K1.sum(output, axis=1)\n\n#loading models (encoder and decoder) and fetching descriptions using the greedy search\nclass Video2Text(object):\n    ''' Initialize the parameters for the model '''\n\n    def __init__(self):\n        self.latent_dim = 512\n        self.num_encoder_tokens = 2048\n        self.num_decoder_tokens = 1500\n        self.time_steps_encoder = 28\n        self.time_steps_decoder = None\n        self.max_probability = -1\n\n        # processed data\n        self.encoder_input_data = []\n        self.decoder_input_data = []\n        self.decoder_target_data = []\n        self.tokenizer = None\n\n        # models\n        self.encoder_model = None\n        self.decoder_model = None\n        self.inf_encoder_model = None\n        self.inf_decoder_model = None\n        self.save_model_path = '/kaggle/input/mp-model-weights'\n        self.test_path_global = 'global_features'\n        self.test_path_motion = 'motion_features1'\n        self.test_path_local = 'local_features1'\n\n    def load_inference_models(self):\n        # load tokenizer\n        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n            self.tokenizer = joblib.load(file)\n            print('tokeniser loaded')\n\n        # inference encoder model\n        self.inf_encoder_model = tf.keras.models.load_model(os.path.join(\n            self.save_model_path, 'encoder_model.h5'), custom_objects={'attention': attention})\n        print('encoder loaded')\n\n        # inference decoder model\n        decoder_inputs = tf.keras.layers.Input(\n            shape=(None, self.num_decoder_tokens))\n        decoder_dense = tf.keras.layers.Dense(\n            self.num_decoder_tokens, activation='softmax')\n\n        decoder_gru = tf.keras.layers.GRU(\n            self.latent_dim, return_sequences=True, return_state=True)\n        decoder_state_input_h = tf.keras.layers.Input(shape=(self.latent_dim,))\n        # decoder_state_input_c = tf.keras.layers.Input(shape=(self.latent_dim,))\n        decoder_states_inputs = [decoder_state_input_h]\n        decoder_outputs, state_h = decoder_gru(\n            decoder_inputs, initial_state=decoder_states_inputs)\n        decoder_states = [state_h]\n        decoder_outputs = decoder_dense(decoder_outputs)\n        self.inf_decoder_model = tf.keras.models.Model(\n            [decoder_inputs] + decoder_states_inputs,\n            [decoder_outputs] + decoder_states)\n\n        self.inf_decoder_model.load_weights(os.path.join(\n            self.save_model_path, 'decoder_model_weights.h5'))\n        print('Loaded Inference Model')\n\n    def index_to_word(self):\n        # inverts word tokenizer\n        index_to_word = {value: key for key,\n                         value in self.tokenizer.word_index.items()}\n        return index_to_word\n\n    def greedy_search(self, f):\n        \"\"\"\n        :param f: the loaded numpy array after creating videos to frames and extracting features\n        :return: the final sentence which has been predicted greedily\n        \"\"\"\n        inv_map = self.index_to_word()\n        states_value = self.inf_encoder_model.predict(f.reshape(-1, 28, 10240))\n        states_value = np.array(states_value).reshape((1, 512))\n        target_seq = np.zeros((1, 1, 1500))\n        sentence = ''\n        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n        for i in range(15):\n            output_tokens, h = self.inf_decoder_model.predict(\n                [target_seq] + [states_value])\n            states_value = [h]\n            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n            y_hat = np.argmax(output_tokens)\n            if y_hat == 0:\n                continue\n            if inv_map[y_hat] is None:\n                break\n            else:\n                sentence = sentence + inv_map[y_hat] + ' '\n                target_seq = np.zeros((1, 1, 1500))\n                target_seq[0, 0, y_hat] = 1\n        return ' '.join(sentence.split()[:-1])\n\n    def test(self, global_features, motion_features):\n        X_test = np.concatenate(\n            (global_features, motion_features, motion_features), axis=1)\n        x = X_test.reshape(-1, 28, 10240)\n        print(x.shape)\n        decoded_sentence = self.greedy_search(x)\n        return (decoded_sentence)\n\n    \n#object of above class    \nc = Video2Text()\nc.load_inference_models()\n\n\n\n#main driver function for generating the descriptions\ndef get_description(path):\n\n    frames = resize_frames(video_to_frames(path, K))\n\n    print(len(frames))\n    print(frames[0].shape)\n\n    \"\"\"# ***Features Extraction***\n\n  # ***Global Features***\n  \"\"\"\n\n    # Extract global features for training\n\n    global_features = extract_global_features(True, path)\n\n    print(len(global_features))\n    print(global_features[0].shape)\n\n    \"\"\"# ***Motion Features***\"\"\"\n\n    # Extracting Motion Features\n\n    motion_features = extract_motion_features(True, path)\n\n    print(motion_features.shape)\n\n    \"\"\"# ***GetDescription***\"\"\"\n\n\n    return (c.test(global_features, motion_features))\n\n\n# print(get_description('content/_0nX-El-ySo_83_93.avi'))\n# print(get_description(\n#     '/kaggle/working/_0nX-El-ySo_83_93.mp4'))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:26:24.919784Z","iopub.execute_input":"2023-04-01T21:26:24.920492Z","iopub.status.idle":"2023-04-01T21:26:32.521283Z","shell.execute_reply.started":"2023-04-01T21:26:24.920455Z","shell.execute_reply":"2023-04-01T21:26:32.519914Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tokeniser loaded\nencoder loaded\nLoaded Inference Model\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Evaluation System For All Searches Made And Recommendations Made**\n\n#### **Precison and Recall  Calculations**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n#function to calculate precision and recall \ndef calculate_precison_recall(search_with_desc):\n    model_transformer2 = SentenceTransformer('all-mpnet-base-v2')\n    df_encoded = model_transformer2.encode(df.Description.values)\n    threshold = 0.5\n    #for search\n    avg_precision = 0.00\n    avg_recall = 0.00\n    for search_term in search_with_desc.keys():\n        search_with_desc[search_term] = search_with_desc[search_term]\n        relev_retrieved = set()\n        total_relev = set()\n        search_term_encode = model_transformer2.encode([search_term])\n        for videoID in  search_with_desc[search_term].keys():\n            desc_encode = model_transformer2.encode([search_with_desc[search_term][videoID]])\n            result = sklearn.metrics.pairwise.cosine_similarity(search_term_encode, desc_encode, dense_output=True)[0]\n            if result >= threshold:\n                relev_retrieved.add(videoID)\n        res = []\n        results = sklearn.metrics.pairwise.cosine_similarity(df_encoded, search_term_encode, dense_output=True)\n        for i in range(len(results)):\n                 res.append([results[i], i])\n            #sorting \n        res.sort(reverse=True)\n        i = 0\n        while(i<len(res) and res[i][0]>=threshold):\n                total_relev.add(df.iloc[res[i][1], 0])\n                i+=1\n        print('reelvant ret : ',len(relev_retrieved))\n        print('total relev : ',len(total_relev) )\n        print('total shown : ',len(search_with_desc[search_term]))\n        try:\n            avg_precision += len(relev_retrieved)/len(search_with_desc[search_term])\n        except:\n            print()\n        try:\n            avg_recall += len(relev_retrieved)/len(total_relev)\n        except:\n            print()\n            \n    avg_precision /= len(search_with_desc)\n    avg_recall /= len(search_with_desc)\n    \n    print('Average Precision : ',avg_precision)\n    print('Average Recall : ',avg_recall)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:26:32.523414Z","iopub.execute_input":"2023-04-01T21:26:32.523910Z","iopub.status.idle":"2023-04-01T21:26:32.538670Z","shell.execute_reply.started":"2023-04-01T21:26:32.523858Z","shell.execute_reply":"2023-04-01T21:26:32.537403Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# **Flask Server**","metadata":{}},{"cell_type":"code","source":"#installing local tunnel to expose local server to the outer world\n!pip install flask-localtunnel\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T21:26:32.541151Z","iopub.execute_input":"2023-04-01T21:26:32.542252Z","iopub.status.idle":"2023-04-01T21:26:44.102256Z","shell.execute_reply.started":"2023-04-01T21:26:32.542199Z","shell.execute_reply":"2023-04-01T21:26:44.100823Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: flask-localtunnel in /opt/conda/lib/python3.7/site-packages (1.0.5)\nRequirement already satisfied: Flask>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from flask-localtunnel) (2.2.3)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.14.0->flask-localtunnel) (4.11.4)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.14.0->flask-localtunnel) (3.1.2)\nRequirement already satisfied: click>=8.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.14.0->flask-localtunnel) (8.1.3)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.14.0->flask-localtunnel) (2.2.3)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask>=0.14.0->flask-localtunnel) (2.1.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.14.0->flask-localtunnel) (3.11.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->Flask>=0.14.0->flask-localtunnel) (4.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->Flask>=0.14.0->flask-localtunnel) (2.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from flask import Flask\nfrom flask import send_from_directory\nfrom flask_ngrok import run_with_ngrok\nfrom distutils.log import debug\nfrom fileinput import filename\nfrom flask import *  \nimport datetime\nimport math\nimport random\nfrom pyngrok import ngrok\nfrom flask_cors import CORS\nfrom flask_lt import run_with_lt#\ndf1 = pd.read_csv('/kaggle/input/datacaptions/data.csv')\n\n\n#use this in case of using ngrok instead of local tunnel\nngrok.set_auth_token(\"2N1pQ8aidKgB9HZeGmFVyvI1wFZ_qB94CGiuXoAE9q5vASKP\")\n\n\n#Storage\n#here for simplicity one user is considered otherwise dictionary of dictionaries can be created for multiple users,\n#where first dictionary will contain userID as key\nuploads_with_desc = {} #storing total uploaded data urls for particular user\n\nuploads = [] #contains file_name\nuploads_weight = [] #contains weight\n\nsearch_with_desc = {} #to store all searches and their results\ntotal_uploads_with_desc = {} #to store all recommendataion to be made  and descriptions generated as key\n\n\nseed_weight = 0.1 #will increase it gradually to give more weight to recent videos\n\n\n#map to check for case of reupload\ncheck_reupload = defaultdict(int)\n\n#initial time\ninitial_time = datetime.datetime(2023, 3, 14, 12, 0, 0)\n\n#App\n\napp = Flask(__name__)\nCORS(app)  #adding support for cors by sending cors allow headers and bypassing tunnel reminder in response by the server\napp.config['CORS_ALLOW_HEADERS'] = ['Content-Type', 'Authorization', 'Bypass-Tunnel-Reminder','bypass-tunnel-reminder']\n\nrun_with_lt(app) #running with local tunnel\n# run_with_ngrok(app) \n\n# @app.after_request\n# def add_cors_headers(response):\n#     response.headers['Access-Control-Allow-Origin'] = '*'\n#     return response\n\n\n#file upload\n@app.route('/upload', methods = ['POST'])  \ndef file_upload():  \n    if request.method == 'POST':  \n        f = request.files['file']\n        f.save(f.filename)  \n        print(f.filename)\n        desc = get_description('/kaggle/working/'+f.filename) \n        print(desc)\n        if check_reupload[desc] == 0:\n            current_time = datetime.datetime.now()\n            time_difference = current_time - initial_time\n            hours_difference = time_difference.total_seconds() / 3600\n            if int(math.ceil(hours_difference))%24 == 0:\n                seed_weight+=0.1\n            uploads_with_desc[f.filename] = desc\n            data,Ids = generate_results(desc)\n            \n            try:\n                total_uploads_with_desc[desc]= data\n            except:\n                print('COULD NOT FILL DICT')\n                \n            uploads.extend(data)\n            print('Current Upload Len : ',len(Ids),' ',len(uploads))\n            temp_weight = []\n            for i in range(len(Ids)):\n                temp_weight.append(seed_weight)\n#             temp_weight = [seed_weight]* 30\n#             print('Temp Weight : ',temp_weight)\n            uploads_weight.extend(temp_weight)\n            check_reupload[desc] = 1\n        return render_template(\"Acknowledgement.html\", name = f.filename)  \n\n#serving static files\n@app.route('/file/<path:path>', methods=['GET'])\ndef send_data(path):\n    print(path)\n    return send_from_directory('/kaggle/input/avi-2-mp4','mp4/'+path+'.mp4')\n\n# fetching results for users search\n@app.route('/search/<name>', methods=['GET'])\ndef search_video(name):\n    if (request.method == 'GET'):\n        print(name)\n        data,_ = generate_results(name)\n        try:\n            search_with_desc[name] = data\n        except:\n            print('Generated Error In Storing Search Results')\n#         print(data.keys())\n        return jsonify(data)\n\n\n\n#serving uploaded videos\n@app.route('/serveUploadedFile/<path:path>', methods=['GET'])\ndef send_data_ofUpload(path):\n    print(path)\n    return send_from_directory('/kaggle/working/',path)\n\n#sending Ids and description of uploaded videos\n@app.route('/uploaded/', methods=['GET'])\ndef uploaded_video_feed_data():\n    return (jsonify(uploads_with_desc))\n\n\n\n#Returning video for recommendations feed\n\n@app.route('/rec/<name>', methods=['GET'])\ndef send_rec_data(name):\n    print('rec feed path : ',name)\n    return send_from_directory('/kaggle/input/avi-2-mp4','mp4/'+ name +'.mp4')\n    \n\n@app.route('/rec_dic/', methods=['GET'])\ndef generate_video_feed_data():\n#     print(uploads)\n    desc={}\n    for id in uploads:\n        try:\n#              print(df[df['VideoID']==str(id)]['VideoID'].iloc[[0]].values[0])\n#              print(df[df['VideoID']==str(id)]['Description'].iloc[[0]].values[0])\n             desc[df1[df1['VideoID']==str(id)]['VideoID'].iloc[[0]].values[0] ] = df1[df1['VideoID']==str(id)]['Description'].iloc[[0]].values[0]\n        except:\n            print('Not Found For ID : ',id)\n#         desc[df.iloc[id, 0]] = df.iloc[id, 1]\n    print(desc)\n    return (jsonify(desc))\n\n\n\nif __name__ == '__main__':\n    app.run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-04-01T21:26:44.104878Z","iopub.execute_input":"2023-04-01T21:26:44.105374Z","iopub.status.idle":"2023-04-01T21:28:39.067902Z","shell.execute_reply.started":"2023-04-01T21:26:44.105324Z","shell.execute_reply":"2023-04-01T21:28:39.066333Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n * Serving Flask app '__main__'\n * Debug mode: off\nyour url is: https://moody-doors-knock-35-201-201-161.loca.lt\n{}\n{}\na man riding in a car\nQUERIED DESC :  a man riding in a car\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58bb11fae36d44af9b810e7d699cb43b"}},"metadata":{}},{"name":"stdout","text":"a man riding in a car\nQUERIED DESC :  a man riding in a car\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a12bb6832b498cb6434596930d1b19"}},"metadata":{}},{"name":"stdout","text":"2YhDTpzxd3c_223_232\n3ftaoFnhTyY_140_150\n778mkceE0UQ_40_46\nAKVBK-0veE8_5_12\nEPXsiQw9vvo_1_12\nIhzXQ-gc9IQ_125_129\nP6eo3LULuuc_1_11\nPslVHtXc7Tg_8_14\nQwn0Ni9G5ow_64_69\nX_NLV2KCnIE_60_70\nZdNP0dXCx2I_78_90\nfjDvKHkmxs0_119_126\nbruzcOyIGeg_4_12\ngHzws6FpuNE_10_12\nnBFhvrAOFqY_23_29\nghynaoVNwZc_1_20\nuGDuIyfJXXg_23_33\nsJC7E06IBXI_49_59\nuVPnDJKt1M0_0_6\nurXDqw3S34I_12_17\n778mkceE0UQ_40_46\nAKVBK-0veE8_5_12\nIhzXQ-gc9IQ_125_129\nEPXsiQw9vvo_1_12\n3ftaoFnhTyY_140_150\n2YhDTpzxd3c_223_232\nP6eo3LULuuc_1_11\nPslVHtXc7Tg_8_14\nQwn0Ni9G5ow_64_69\nX_NLV2KCnIE_60_70\nZdNP0dXCx2I_78_90\nfjDvKHkmxs0_119_126\nbruzcOyIGeg_4_12\ngHzws6FpuNE_10_12\nnBFhvrAOFqY_23_29\nghynaoVNwZc_1_20\nuGDuIyfJXXg_23_33\nsJC7E06IBXI_49_59\nuVPnDJKt1M0_0_6\nurXDqw3S34I_12_17\n2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"***Add Garbage Collector For RAM Optimisations***","metadata":{}},{"cell_type":"markdown","source":"# **Evaluation results**","metadata":{}},{"cell_type":"code","source":"calculate_precison_recall(search_with_desc)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calculate_precison_recall(total_uploads_with_desc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
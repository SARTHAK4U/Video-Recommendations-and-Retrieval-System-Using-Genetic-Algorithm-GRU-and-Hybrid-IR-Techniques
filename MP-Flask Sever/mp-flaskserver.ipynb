{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installations And Pretrained Models File Fetch**","metadata":{}},{"cell_type":"markdown","source":"Installing libraries and important files","metadata":{}},{"cell_type":"code","source":"!pip install -U flask-cors\n!pip install flask-ngrok\n!pip install pyngrok\n!pip install -U sentence_transformers\n!pip install keras-preprocessing\n!pip install sk-video\n!wget -O 'c3d.py' 'https://drive.google.com/uc?id=1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j&confirm=t'\n!wget -O 'sports1M_utils.py' 'https://drive.google.com/uc?id=1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i&confirm=t'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Recommendations And Search Cell**","metadata":{}},{"cell_type":"code","source":"#recommender cell\nfrom flask import Flask\nfrom flask_ngrok import run_with_ngrok\nimport numpy as np\nfrom collections import defaultdict\nimport tqdm.notebook as tqdm #for loader graphic\nimport networkx as nx\n# import torch\nfrom sentence_transformers import SentenceTransformer, util\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport os\nimport re\nimport collections\nimport nltk\nimport numpy as mp\nimport pandas as pd\nfrom flask import request\nfrom flask import jsonify\ndf = pd.read_csv('/kaggle/input/datacaptions/data.csv')\ndf = df.groupby('VideoID', as_index=False).first()\n\n\n# len(df)\n\n\n#data cleaning of the descriptions\nnltk.download('punkt')\n# import requests\n# import bs4\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\n\ndf.head()\n\n\n#creating instance of sentence transformer\nmodel_transformer = SentenceTransformer('paraphrase-MiniLM-L6-v2')\nX = model_transformer.encode(df.Description.values)\n\ndef generate_results(test_sentence, status=1):\n    print('QUERIED DESC : ',test_sentence)\n    data = {}\n    Ids = []\n    \n    #encoding query vector\n    query_vec = model_transformer.encode(test_sentence)\n\n    #finding cosine similarity with vectors of database\n    results = sklearn.metrics.pairwise.cosine_similarity(\n        X, [query_vec], dense_output=True)\n\n    res = []\n    for i in range(len(results)):\n        res.append([results[i], i])\n   #sorting \n    res.sort(reverse=True)\n    if status == 0:\n        return res\n    # print('Recommended Video--------Recommendation')\n    \n    #returning top30 information retrieval results\n    for i in res[:30]:\n        data[df.iloc[i[1], 0]] = df.iloc[i[1], 1]\n        Ids.append(df.iloc[i[1], 0])\n        # print(df.iloc[i[1], 0], \"--------\", df.iloc[i[1], 1])\n    return data,Ids\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Video Description Generator**","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\n\nfrom tensorflow import keras\nfrom keras_preprocessing.sequence import pad_sequences\nfrom keras_preprocessing.text import Tokenizer\nfrom tensorflow.keras import backend as K1\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.applications.inception_v3 import InceptionV3\nimport os\nimport cv2\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport skvideo.io\nimport importlib as imp\nfrom c3d import C3D\nfrom keras.models import Model\nimport sports1M_utils\n\n\"\"\"# ***Video PreProcessing***\"\"\"\n\nK = 28\n\n\n#function to fetch equally spaced k frames\ndef extract_frames_equally_spaced(frames, K):\n    n_frames = len(frames)\n\n    splits = np.array_split(range(n_frames), K)\n    idx_taken = [s[0] for s in splits]\n    sub_frames = []\n\n    for idx in idx_taken:\n        sub_frames.append(frames[idx])\n    return sub_frames\n\n#padding frames to make net frames same for all\ndef pad_frames(frames, limit, jpegs=False):\n    last_frame = frames[-1]\n    if jpegs:\n        frames_padded = frames + [last_frame]*(limit-len(frames))\n    else:\n        padding = np.asarray([last_frame * 0.]*(limit-len(frames)))\n        frames_padded = np.concatenate([frames, padding], axis=0)\n    return frames_padded\n\n#resizing of the frames\ndef resize_frames(frames):\n    new_frames = []\n    for frame in frames:\n        new_frame = cv2.resize(frame, (224, 224))\n        new_frames.append(new_frame)\n\n    return new_frames\n\n#extracting k frames from the videos\ndef video_to_frames(input_loc, K):\n    cap = cv2.VideoCapture(input_loc)\n\n    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n    count = 0\n\n    frames = []\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            continue\n        frames.append(frame)\n        count = count + 1\n\n        if (count > (video_length-1)):\n            cap.release()\n            break\n    if len(frames) < K:\n        frames = pad_frames(frames, K, True)\n    else:\n        frames = extract_frames_equally_spaced(frames, K)\n\n    return frames\n\n\n#model for global feature extraction\ndef get_model():\n    i = tf.keras.layers.Input([224, 224, 3], dtype=tf.float32)\n    x = tf.keras.applications.inception_v3.preprocess_input(i)\n    model = InceptionV3(weights=\"imagenet\", include_top=False,\n                        input_shape=(224, 224, 3), pooling='avg')\n    out = model(x)\n    model = tf.keras.models.Model(inputs=[i], outputs=out)\n    return model\n    # model.summary()\n\n\n\ngoogle_net_model = get_model()\n\n#extracting global-features from the frames\ndef extract_global_features(save, path):\n    if not os.path.exists('global_features'):\n        os.makedirs('global_features')\n\n    frames = video_to_frames(path, K)\n    frames = resize_frames(frames)\n    frames = np.array(frames)\n\n    feature_vector = google_net_model.predict(frames, batch_size=128)\n    feature_path = 'global_features/'+'global'+'.npy'\n    np.save(feature_path, feature_vector)\n    return feature_vector\n\n\n\n#extracting motion features from the frames\ndef extract_frames_equally_spaced_motion(frames, K):\n    n_frames = len(frames)\n    if (n_frames < K):\n        while len(frames) < K:\n            frames.append(frames[-1])\n        return frames\n\n    # print('Nframes : ',n_frames)\n    splits = np.array_split(range(n_frames), K)\n    idx_taken = [s[0] for s in splits]\n    sub_frames = []\n\n    for idx in idx_taken:\n        sub_frames.append(frames[idx])\n    return sub_frames\n\n\n#creating instance of pretrained model\nbase_model = C3D(weights='sports1M')\nmodel_motion = Model(inputs=base_model.input,\n                  outputs=base_model.get_layer('fc6').output)\n\n\n# extracting motion features over windows using previously defined functions\ndef extract_motion_features(save, path):\n    imp.reload(sports1M_utils)\n    if not os.path.exists('motion_features1'):\n        os.makedirs('motion_features1')\n\n\n    \n    frames = video_to_frames(path, K)\n    frames = resize_frames(frames)\n    frames = np.array(frames)\n\n    windows = []\n    org_vid = frames\n    vid = []\n    motion_features_video = []\n    for i in range(len(frames)):\n        vid.append(frames[i])\n        if i == 15:\n            windows.append(vid)\n        elif i >= 16:\n            vid.pop(0)\n            windows.append(vid)\n    print(len(windows))\n    print(len(windows[0]))\n\n    windows = extract_frames_equally_spaced_motion(windows, 28)\n    for window in windows:\n        x = sports1M_utils.preprocess_input(np.array(window))\n        feature_vector = model_motion.predict(x)\n        feature_vector = feature_vector[0]\n        feature_vector = np.array(feature_vector)\n\n        motion_features_video.append(feature_vector)\n    motion_features_video = np.array(motion_features_video)\n\n    path = 'motion_features1/'+'motion'+'.npy'\n    np.save(path, motion_features_video)\n    return motion_features_video\n\n\n\n#attention mechanism class used to create model skeltal and load the weights obtained from the training of model\nclass attention(tf.keras.layers.Layer):\n\n    def __init__(self, return_sequences=True, **kwargs):\n        self.return_sequences = return_sequences\n        super(attention, self).__init__()\n\n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'return_sequences': self.return_sequences\n        })\n        return config\n\n    def build(self, input_shape):\n\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n\n        super(attention, self).build(input_shape)\n\n    def call(self, x):\n\n        e = K1.tanh(K1.dot(x, self.W)+self.b)\n        a = K1.softmax(e, axis=1)\n        output = x*a\n\n        if self.return_sequences:\n            return output\n\n        return K1.sum(output, axis=1)\n\n#loading models (encoder and decoder) and fetching descriptions using the greedy search\nclass Video2Text(object):\n    ''' Initialize the parameters for the model '''\n\n    def __init__(self):\n        self.latent_dim = 512\n        self.num_encoder_tokens = 2048\n        self.num_decoder_tokens = 1500\n        self.time_steps_encoder = 28\n        self.time_steps_decoder = None\n        self.max_probability = -1\n\n        # processed data\n        self.encoder_input_data = []\n        self.decoder_input_data = []\n        self.decoder_target_data = []\n        self.tokenizer = None\n\n        # models\n        self.encoder_model = None\n        self.decoder_model = None\n        self.inf_encoder_model = None\n        self.inf_decoder_model = None\n        self.save_model_path = '/kaggle/input/mp-model-weights'\n        self.test_path_global = 'global_features'\n        self.test_path_motion = 'motion_features1'\n        self.test_path_local = 'local_features1'\n\n    def load_inference_models(self):\n        # load tokenizer\n        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n            self.tokenizer = joblib.load(file)\n            print('tokeniser loaded')\n\n        # inference encoder model\n        self.inf_encoder_model = tf.keras.models.load_model(os.path.join(\n            self.save_model_path, 'encoder_model.h5'), custom_objects={'attention': attention})\n        print('encoder loaded')\n\n        # inference decoder model\n        decoder_inputs = tf.keras.layers.Input(\n            shape=(None, self.num_decoder_tokens))\n        decoder_dense = tf.keras.layers.Dense(\n            self.num_decoder_tokens, activation='softmax')\n\n        decoder_gru = tf.keras.layers.GRU(\n            self.latent_dim, return_sequences=True, return_state=True)\n        decoder_state_input_h = tf.keras.layers.Input(shape=(self.latent_dim,))\n        # decoder_state_input_c = tf.keras.layers.Input(shape=(self.latent_dim,))\n        decoder_states_inputs = [decoder_state_input_h]\n        decoder_outputs, state_h = decoder_gru(\n            decoder_inputs, initial_state=decoder_states_inputs)\n        decoder_states = [state_h]\n        decoder_outputs = decoder_dense(decoder_outputs)\n        self.inf_decoder_model = tf.keras.models.Model(\n            [decoder_inputs] + decoder_states_inputs,\n            [decoder_outputs] + decoder_states)\n\n        self.inf_decoder_model.load_weights(os.path.join(\n            self.save_model_path, 'decoder_model_weights.h5'))\n        print('Loaded Inference Model')\n\n    def index_to_word(self):\n        # inverts word tokenizer\n        index_to_word = {value: key for key,\n                         value in self.tokenizer.word_index.items()}\n        return index_to_word\n\n    def greedy_search(self, f):\n        \"\"\"\n        :param f: the loaded numpy array after creating videos to frames and extracting features\n        :return: the final sentence which has been predicted greedily\n        \"\"\"\n        inv_map = self.index_to_word()\n        states_value = self.inf_encoder_model.predict(f.reshape(-1, 28, 10240))\n        states_value = np.array(states_value).reshape((1, 512))\n        target_seq = np.zeros((1, 1, 1500))\n        sentence = ''\n        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n        for i in range(15):\n            output_tokens, h = self.inf_decoder_model.predict(\n                [target_seq] + [states_value])\n            states_value = [h]\n            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n            y_hat = np.argmax(output_tokens)\n            if y_hat == 0:\n                continue\n            if inv_map[y_hat] is None:\n                break\n            else:\n                sentence = sentence + inv_map[y_hat] + ' '\n                target_seq = np.zeros((1, 1, 1500))\n                target_seq[0, 0, y_hat] = 1\n        return ' '.join(sentence.split()[:-1])\n\n    def test(self, global_features, motion_features):\n        X_test = np.concatenate(\n            (global_features, motion_features, motion_features), axis=1)\n        x = X_test.reshape(-1, 28, 10240)\n        print(x.shape)\n        decoded_sentence = self.greedy_search(x)\n        return (decoded_sentence)\n\n    \n#object of above class    \nc = Video2Text()\nc.load_inference_models()\n\n\n\n#main driver function for generating the descriptions\ndef get_description(path):\n\n    frames = resize_frames(video_to_frames(path, K))\n\n    print(len(frames))\n    print(frames[0].shape)\n\n    \"\"\"# ***Features Extraction***\n\n  # ***Global Features***\n  \"\"\"\n\n    # Extract global features for training\n\n    global_features = extract_global_features(True, path)\n\n    print(len(global_features))\n    print(global_features[0].shape)\n\n    \"\"\"# ***Motion Features***\"\"\"\n\n    # Extracting Motion Features\n\n    motion_features = extract_motion_features(True, path)\n\n    print(motion_features.shape)\n\n    \"\"\"# ***GetDescription***\"\"\"\n\n\n    return (c.test(global_features, motion_features))\n\n\n# print(get_description('content/_0nX-El-ySo_83_93.avi'))\n# print(get_description(\n#     '/kaggle/working/_0nX-El-ySo_83_93.mp4'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Flask Server**","metadata":{}},{"cell_type":"code","source":"#installing local tunnel to expose local server to the outer world\n!pip install flask-localtunnel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flask import Flask\nfrom flask import send_from_directory\nfrom flask_ngrok import run_with_ngrok\nfrom distutils.log import debug\nfrom fileinput import filename\nfrom flask import *  \nimport datetime\nimport math\nimport random\nfrom pyngrok import ngrok\nfrom flask_cors import CORS\nfrom flask_lt import run_with_lt#\ndf1 = pd.read_csv('/kaggle/input/datacaptions/data.csv')\n\n\n#use this in case of using ngrok instead of local tunnel\nngrok.set_auth_token(\"2N1pQ8aidKgB9HZeGmFVyvI1wFZ_qB94CGiuXoAE9q5vASKP\")\n\n\n#Storage\n#here for simplicity one user is considered otherwise dictionary of dictionaries can be created for multiple users,\n#where first dictionary will contain userID as key\nuploads_with_desc = {} #storing total uploaded data urls for particular user\n\nuploads = [] #contains file_name\nuploads_weight = [] #contains weight\n\n\n\nseed_weight = 0.1 #will increase it gradually to give more weight to recent videos\n\n\n#map to check for case of reupload\ncheck_reupload = defaultdict(int)\n\n#initial time\ninitial_time = datetime.datetime(2023, 3, 14, 12, 0, 0)\n\n#App\n\napp = Flask(__name__)\nCORS(app)  #adding support for cors by sending cors allow headers and bypassing tunnel reminder in response by the server\napp.config['CORS_ALLOW_HEADERS'] = ['Content-Type', 'Authorization', 'Bypass-Tunnel-Reminder','bypass-tunnel-reminder']\n\nrun_with_lt(app) #running with local tunnel\n# run_with_ngrok(app) \n\n# @app.after_request\n# def add_cors_headers(response):\n#     response.headers['Access-Control-Allow-Origin'] = '*'\n#     return response\n\n\n#file upload\n@app.route('/upload', methods = ['POST'])  \ndef file_upload():  \n    if request.method == 'POST':  \n        f = request.files['file']\n        f.save(f.filename)  \n        print(f.filename)\n        desc = get_description('/kaggle/working/'+f.filename) \n        print(desc)\n        if check_reupload[desc] == 0:\n            current_time = datetime.datetime.now()\n            time_difference = current_time - initial_time\n            hours_difference = time_difference.total_seconds() / 3600\n            if int(math.ceil(hours_difference))%24 == 0:\n                seed_weight+=0.1\n            uploads_with_desc[f.filename] = desc\n            data,Ids = generate_results(desc)\n            \n            uploads.extend(data)\n            print('Current Upload Len : ',len(Ids),' ',len(uploads))\n            temp_weight = []\n            for i in range(len(Ids)):\n                temp_weight.append(seed_weight)\n#             temp_weight = [seed_weight]* 30\n#             print('Temp Weight : ',temp_weight)\n            uploads_weight.extend(temp_weight)\n            check_reupload[desc] = 1\n        return render_template(\"Acknowledgement.html\", name = f.filename)  \n\n#serving static files\n@app.route('/file/<path:path>', methods=['GET'])\ndef send_data(path):\n    print(path)\n    return send_from_directory('/kaggle/input/avi-2-mp4','mp4/'+path+'.mp4')\n\n# fetching results for users search\n@app.route('/search/<name>', methods=['GET'])\ndef search_video(name):\n    if (request.method == 'GET'):\n        print(name)\n        data,_ = generate_results(name)\n#         print(data.keys())\n        return jsonify(data)\n\n\n\n#serving uploaded videos\n@app.route('/serveUploadedFile/<path:path>', methods=['GET'])\ndef send_data_ofUpload(path):\n    print(path)\n    return send_from_directory('/kaggle/working/',path)\n\n#sending Ids and description of uploaded videos\n@app.route('/uploaded/', methods=['GET'])\ndef uploaded_video_feed_data():\n    return (jsonify(uploads_with_desc))\n\n\n\n#Returning video for recommendations feed\n\n@app.route('/rec/<name>', methods=['GET'])\ndef send_rec_data(name):\n    print('rec feed path : ',name)\n    return send_from_directory('/kaggle/input/avi-2-mp4','mp4/'+ name +'.mp4')\n    \n\n@app.route('/rec_dic/', methods=['GET'])\ndef generate_video_feed_data():\n#     print(uploads)\n    desc={}\n    for id in uploads:\n        try:\n#              print(df[df['VideoID']==str(id)]['VideoID'].iloc[[0]].values[0])\n#              print(df[df['VideoID']==str(id)]['Description'].iloc[[0]].values[0])\n             desc[df1[df1['VideoID']==str(id)]['VideoID'].iloc[[0]].values[0] ] = df1[df1['VideoID']==str(id)]['Description'].iloc[[0]].values[0]\n        except:\n            print('Not Found For ID : ',id)\n#         desc[df.iloc[id, 0]] = df.iloc[id, 1]\n    print(desc)\n    return (jsonify(desc))\n\n\n\nif __name__ == '__main__':\n    app.run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Add Garbage Collector For RAM Optimisations***","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
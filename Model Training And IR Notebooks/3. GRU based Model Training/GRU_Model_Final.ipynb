{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Do0HUTqKkSrO",
        "outputId": "73e729d9-c34c-4cdc-bdd9-5a3d53a2d4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yCLBJl8ikyUw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import functools, operator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lb10JJYTk0mO",
        "outputId": "22ad4769-099d-49a8-babc-05ab6b191b58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7b35b9f2-e127-451a-b43b-8e626ad75a2f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VideoID</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-4wsuPCjDBc_5_15</td>\n",
              "      <td>a squirrel is eating a peanut in it s shell</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-4wsuPCjDBc_5_15</td>\n",
              "      <td>a chipmunk is eating</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-4wsuPCjDBc_5_15</td>\n",
              "      <td>a chipmunk is eating a peanut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4wsuPCjDBc_5_15</td>\n",
              "      <td>a chipmunk is eating a nut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-4wsuPCjDBc_5_15</td>\n",
              "      <td>a squirrel is eating a nut</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80783</th>\n",
              "      <td>zxB4dFJhHR8_1_9</td>\n",
              "      <td>a girl riding a bicycle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80784</th>\n",
              "      <td>zxB4dFJhHR8_1_9</td>\n",
              "      <td>a smiling girl wearing backpack is riding a bike</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80785</th>\n",
              "      <td>zxB4dFJhHR8_1_9</td>\n",
              "      <td>the girl rode her bike at the beach</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80786</th>\n",
              "      <td>zzit5b_-ukg_5_20</td>\n",
              "      <td>a boy is doing exercise by cycle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80787</th>\n",
              "      <td>zzit5b_-ukg_5_20</td>\n",
              "      <td>a boy is doing exercise on machine</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80788 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b35b9f2-e127-451a-b43b-8e626ad75a2f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b35b9f2-e127-451a-b43b-8e626ad75a2f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b35b9f2-e127-451a-b43b-8e626ad75a2f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                VideoID                                       Description\n",
              "0      -4wsuPCjDBc_5_15       a squirrel is eating a peanut in it s shell\n",
              "1      -4wsuPCjDBc_5_15                              a chipmunk is eating\n",
              "2      -4wsuPCjDBc_5_15                     a chipmunk is eating a peanut\n",
              "3      -4wsuPCjDBc_5_15                        a chipmunk is eating a nut\n",
              "4      -4wsuPCjDBc_5_15                        a squirrel is eating a nut\n",
              "...                 ...                                               ...\n",
              "80783   zxB4dFJhHR8_1_9                           a girl riding a bicycle\n",
              "80784   zxB4dFJhHR8_1_9  a smiling girl wearing backpack is riding a bike\n",
              "80785   zxB4dFJhHR8_1_9               the girl rode her bike at the beach\n",
              "80786  zzit5b_-ukg_5_20                  a boy is doing exercise by cycle\n",
              "80787  zzit5b_-ukg_5_20                a boy is doing exercise on machine\n",
              "\n",
              "[80788 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/MP/data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YLs-l-_uk01-",
        "outputId": "972880b5-4192-4852-9e52-ee9b4b621f68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1970\n"
          ]
        }
      ],
      "source": [
        "videoIDs = list(df.VideoID.unique())\n",
        "print(len(videoIDs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dw9H5Sk6k9_q"
      },
      "outputs": [],
      "source": [
        "def extract_frames_equally_spaced(frames, K):\n",
        "        n_frames = len(frames)\n",
        "        splits = np.array_split(range(n_frames), K)\n",
        "        idx_taken = [s[0] for s in splits]\n",
        "        sub_frames = []\n",
        "\n",
        "        for idx in idx_taken:\n",
        "          sub_frames.append(frames[idx])\n",
        "        return sub_frames\n",
        "\n",
        "def pad_frames(frames, limit, jpegs=False):\n",
        "        last_frame = frames[-1]\n",
        "        if jpegs:\n",
        "            frames_padded = frames + [last_frame]*(limit-len(frames))\n",
        "        else:\n",
        "            padding = np.asarray([last_frame * 0.]*(limit-len(frames)))\n",
        "            frames_padded = np.concatenate([frames, padding], axis=0)\n",
        "        return frames_padded\n",
        "\n",
        "def video_to_frames(input_loc,K,motion=False):\n",
        "    cap = cv2.VideoCapture(input_loc)\n",
        "    \n",
        "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
        "    count = 0\n",
        "\n",
        "    if motion is True:\n",
        "      K = max(video_length,50)\n",
        "\n",
        "    frames=[]\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frames.append(frame)\n",
        "        count = count + 1\n",
        "        \n",
        "        if (count > (video_length-1)):\n",
        "            cap.release()\n",
        "            break\n",
        "    if len(frames) < K:\n",
        "        frames = pad_frames(frames,K,True)\n",
        "    else:\n",
        "        frames = extract_frames_equally_spaced(frames,K)\n",
        "    \n",
        "    return frames\n",
        "\n",
        "def resize_frames(frames):\n",
        "  new_frames = []\n",
        "  for frame in frames:\n",
        "    new_frame = cv2.resize(frame,(224,224))\n",
        "    new_frames.append(new_frame)\n",
        "  \n",
        "  return new_frames\n",
        "\n",
        "K=28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qf3tX5U6lwLU"
      },
      "outputs": [],
      "source": [
        "#Googlenet model\n",
        "def get_model():\n",
        "  i = tf.keras.layers.Input([224, 224, 3], dtype = tf.float32)\n",
        "  x = tf.keras.applications.inception_v3.preprocess_input(i)\n",
        "  model = InceptionV3(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3),pooling='avg')\n",
        "  out = model(x)\n",
        "  model=tf.keras.models.Model(inputs=[i], outputs=out)\n",
        "  return model\n",
        "  # model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gIyt3YKFloYJ"
      },
      "outputs": [],
      "source": [
        "#Extract global features for training \n",
        "\n",
        "def extract_global_features(save=False):\n",
        "  if not os.path.exists('global_features'):\n",
        "    os.makedirs('global_features')\n",
        "\n",
        "  model = get_model()\n",
        "\n",
        "  for index, id in enumerate(videoIDs):\n",
        "    path = 'drive/MyDrive/it416/YouTubeClips/'\n",
        "    path+=id\n",
        "    path+='.avi'\n",
        "\n",
        "    frames = video_to_frames(path,K)\n",
        "    frames = resize_frames(frames)\n",
        "    frames = np.array(frames)\n",
        "\n",
        "    feature_vector=model.predict(frames,batch_size=128)\n",
        "    feature_path='global_features/'+id+'.npy'\n",
        "    np.save(feature_path, feature_vector)\n",
        "\n",
        "    print(index,':',id)\n",
        "\n",
        "  if save is True:\n",
        "    !zip -r global_features.zip global_features\n",
        "    !cp 'global_features.zip' 'drive/MyDrive/it416/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M9vJHjGBo95z",
        "outputId": "9a9a66ac-7ad7-486b-9674-2fc2e842ea72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-bcac9c90e399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_global_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-1abc82b47fda>\u001b[0m in \u001b[0;36mextract_global_features\u001b[0;34m(save)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m'.avi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_to_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b0479a34c821>\u001b[0m in \u001b[0;36mvideo_to_frames\u001b[0;34m(input_loc, K, motion)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_frames_equally_spaced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b0479a34c821>\u001b[0m in \u001b[0;36mpad_frames\u001b[0;34m(frames, limit, jpegs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjpegs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mlast_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjpegs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mframes_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlast_frame\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "extract_global_features(save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wF86Y1_AyCxt"
      },
      "outputs": [],
      "source": [
        "!pip install sk-video\n",
        "!wget -O 'c3d.py' 'https://drive.google.com/uc?id=1RC9trsgIo2OsM8dV7CPg73drqSRZMr8j&confirm=t'\n",
        "!wget -O 'sports1M_utils.py' 'https://drive.google.com/uc?id=1S49OSW2pCUPPD9F-763mG-aBx2gBOr2i&confirm=t'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xz0brMxDzg-o"
      },
      "outputs": [],
      "source": [
        "# Extracting Motion Features\n",
        "import skvideo.io\n",
        "import importlib as imp\n",
        "from c3d import C3D\n",
        "from keras.models import Model\n",
        "import sports1M_utils\n",
        "imp.reload(sports1M_utils)\n",
        "\n",
        "def extract_motion_features(save=False):\n",
        "  if not os.path.exists('motion_features1'):\n",
        "    os.makedirs('motion_features1')\n",
        "\n",
        "  base_model = C3D(weights='sports1M')\n",
        "  model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc6').output)\n",
        "\n",
        "  for index, id in enumerate(videoIDs[1600:]):\n",
        "    path = 'drive/MyDrive/IT416/YouTubeClips/'\n",
        "    path+=id\n",
        "    path+='.avi'\n",
        "\n",
        "\n",
        "    frames = video_to_frames(path,K,True)\n",
        "    frames = resize_frames(frames)\n",
        "    frames = np.array(frames)\n",
        "\n",
        "    windows = []\n",
        "    org_vid = frames\n",
        "    vid = []\n",
        "    motion_features_video = []\n",
        "    for i in range(len(frames)):\n",
        "      vid.append(frames[i])\n",
        "      if i==15:\n",
        "        windows.append(vid)\n",
        "      elif i>=16:\n",
        "        vid.pop(0)\n",
        "        windows.append(vid)\n",
        "        \n",
        "    windows = extract_frames_equally_spaced(windows,28)\n",
        "    for window in windows:\n",
        "      x = sports1M_utils.preprocess_input(np.array(window))\n",
        "      feature_vector = model.predict(x) \n",
        "      feature_vector = feature_vector[0]\n",
        "      feature_vector = np.array(feature_vector)\n",
        "\n",
        "      motion_features_video.append(feature_vector)\n",
        "    motion_features_video = np.array(motion_features_video)\n",
        "    \n",
        "    path='motion_features1/'+id+'.npy'\n",
        "    np.save(path, motion_features_video)\n",
        "\n",
        "    if index%200==0 and index>0:\n",
        "      !zip -r motion_features1.zip motion_features1\n",
        "      !cp 'motion_features1.zip' 'drive/MyDrive/IT416/'\n",
        "    print(index,':',motion_features_video.shape)\n",
        "\n",
        "  if save is True:\n",
        "    !zip -r motion_features1.zip motion_features1\n",
        "    !cp 'motion_features1.zip' 'drive/MyDrive/IT416/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fCTTrf6ayO9l"
      },
      "outputs": [],
      "source": [
        "extract_motion_features(save=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3xOK4cQJXZvp"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/MP/global_features.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ID648lRvSQ1d"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/MP/motion_features1.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZChSOipgMJIq"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/MyDrive/MP/local_features1.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fmgt9_QUqmQf"
      },
      "outputs": [],
      "source": [
        "#Prepare training and validation data\n",
        "df = pd.read_csv('/content/drive/MyDrive/MP/data.csv')\n",
        "train_ids = videoIDs[:1870]\n",
        "train_split = 0.85\n",
        "train_list=[]\n",
        "vocab_list=[]\n",
        "for i in range(len(train_ids)):\n",
        "  df_temp=df.loc[df['VideoID']==train_ids[i]]\n",
        "  for _, row in df_temp.iterrows():\n",
        "    caption = \"<bos> \" + row['Description'] + \" <eos>\"\n",
        "    if len(caption.split()) > 10 or len(caption.split()) < 6:\n",
        "        continue\n",
        "    else:\n",
        "        train_list.append([caption, train_ids[i]])\n",
        "random.shuffle(train_list)\n",
        "training_list = train_list[:int(len(train_list)*train_split)]\n",
        "validation_list = train_list[int(len(train_list)*train_split):]\n",
        "\n",
        "for train in training_list:\n",
        "    vocab_list.append(train[0])\n",
        "tokenizer = Tokenizer(num_words=1500)\n",
        "tokenizer.fit_on_texts(vocab_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JEjPBt0NWIF"
      },
      "outputs": [],
      "source": [
        "x_data_global={}\n",
        "for i,id in enumerate(train_ids):\n",
        "  feat_path = 'global_features/'+id+'.npy'\n",
        "  features=np.load(feat_path)\n",
        "  x_data_global[id]=features\n",
        "print(len(x_data_global))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJB0VDS_S6ga"
      },
      "outputs": [],
      "source": [
        "x_data_motion={}\n",
        "for i,id in enumerate(train_ids):\n",
        "  feat_path = 'motion_features1/'+id+'.npy'\n",
        "  features=np.load(feat_path)\n",
        "  x_data_motion[id]=features\n",
        "print(len(x_data_motion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mgxbhLvMW9L"
      },
      "outputs": [],
      "source": [
        "x_data_local={}\n",
        "for i,id in enumerate(train_ids):\n",
        "  feat_path = 'local_features1/'+id+'.npy'\n",
        "  features=np.load(feat_path)\n",
        "  x_data_local[id]=features\n",
        "print(len(x_data_local))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-ij93tPP34F"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Hyperparameters\n",
        "time_steps_encoder=28\n",
        "num_encoder_tokens=2048\n",
        "latent_dim=512\n",
        "time_steps_decoder=10\n",
        "num_decoder_tokens=1500\n",
        "batch_size=320\n",
        "epochs = 25\n",
        "save_model_path = 'model_final'\n",
        "\n",
        "#Data generator for loading data in batches while training\n",
        "def load_dataset(training_list):\n",
        "  encoder_input_data_global = []\n",
        "  encoder_input_data_motion = []\n",
        "  encoder_input_data_local = []\n",
        "  decoder_input_data = []\n",
        "  decoder_target_data = []\n",
        "  train_videoIDs = []\n",
        "  videoSeq = []\n",
        "  for idx, cap in enumerate(training_list):\n",
        "    caption = cap[0]\n",
        "    train_videoIDs.append(cap[1])\n",
        "    videoSeq.append(caption)\n",
        "  train_sequences = tokenizer.texts_to_sequences(videoSeq)\n",
        "  train_sequences = np.array(train_sequences)\n",
        "  train_sequences = pad_sequences(train_sequences, padding='post', truncating='post',\n",
        "                                  maxlen=10)\n",
        "  file_size = len(train_sequences)\n",
        "  n = 0\n",
        "  for i in range(epochs): #epochs\n",
        "    for idx in range(0, file_size):\n",
        "      n += 1\n",
        "      encoder_input_data_global.append(x_data_global[train_videoIDs[idx]])\n",
        "      encoder_input_data_motion.append(x_data_motion[train_videoIDs[idx]])\n",
        "      # encoder_input_data_local.append(x_data_local[train_videoIDs[idx]])\n",
        "      y = tf.keras.utils.to_categorical(train_sequences[idx], 1500)\n",
        "      decoder_input_data.append(y[:-1])\n",
        "      decoder_target_data.append(y[1:])\n",
        "      if n == batch_size: #batch size\n",
        "        encoder_input_global = np.array(encoder_input_data_global)\n",
        "        encoder_input_motion = np.array(encoder_input_data_motion)\n",
        "        # encoder_input_local = np.array(encoder_input_data_local)\n",
        "        decoder_input = np.array(decoder_input_data)\n",
        "        decoder_target = np.array(decoder_target_data)\n",
        "        encoder_input_data_global = []\n",
        "        encoder_input_data_motion = []\n",
        "        # encoder_input_data_local = []\n",
        "        decoder_input_data = []\n",
        "        decoder_target_data = []\n",
        "        n = 0\n",
        "        yield ([encoder_input_global, encoder_input_motion, encoder_input_motion, decoder_input], decoder_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDslP5Ct6A2p"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras import backend as K\n",
        "class attention(tf.keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, return_sequences=True,**kwargs):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(attention,self).__init__()\n",
        "\n",
        "    def get_config(self):\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'return_sequences': self.return_sequences \n",
        "      })\n",
        "      return config\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "        \n",
        "        super(attention,self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        \n",
        "        return K.sum(output, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8opdevEwQZ4_"
      },
      "outputs": [],
      "source": [
        "# Model training\n",
        "def train_model():\n",
        "  encoder_inputs_global = tf.keras.layers.Input(shape=(time_steps_encoder, 2048), name=\"encoder_inputs_global\")\n",
        "  encoder_inputs_motion = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_motion\")\n",
        "  encoder_inputs_local = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_local\")\n",
        "  #add concat layer\n",
        "  encoder_inputs = tf.keras.layers.Concatenate(axis=2)([encoder_inputs_global, encoder_inputs_motion, encoder_inputs_local])\n",
        "\n",
        "  encoder = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='encoder_gru')\n",
        "  # _, state_h = encoder(encoder_inputs)\n",
        "\n",
        "  encoder_seq_output, state_h = encoder(encoder_inputs)\n",
        "  attention_output = attention(return_sequences=False)(encoder_seq_output)\n",
        "  # encoder = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='encoder_lstm')\n",
        "  # _, state_h_1 = encoder(attention_output)\n",
        "  encoder_states = [attention_output]\n",
        "\n",
        "  # encoder_states = [state_h]\n",
        "\n",
        "  decoder_inputs = tf.keras.layers.Input(shape=(time_steps_decoder, num_decoder_tokens), name=\"decoder_inputs\")\n",
        "  decoder_gru = tf.keras.layers.GRU(latent_dim, return_sequences=True, return_state=True, name='decoder_gru')\n",
        "  decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
        "  decoder_outputs = tf.keras.layers.Dropout(0.3)(decoder_outputs)\n",
        "  decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='sigmoid', name='decoder_sigmoid')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "  model = tf.keras.models.Model([encoder_inputs_global,encoder_inputs_motion, encoder_inputs_local, decoder_inputs], decoder_outputs)\n",
        "  # model.summary()\n",
        "  # training_list, validation_list = preprocessing()\n",
        "\n",
        "  train = load_dataset(training_list)\n",
        "  valid = load_dataset(validation_list)\n",
        "\n",
        "  # early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='min')\n",
        "\n",
        "  # Run training\n",
        "  opt = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
        "  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                factor=0.1, patience=5, verbose=1,\n",
        "                                                mode=\"auto\")\n",
        "  model.compile(metrics=['accuracy'], optimizer=opt, loss='categorical_crossentropy')\n",
        "\n",
        "  validation_steps = len(validation_list)//batch_size\n",
        "  steps_per_epoch = len(training_list)//batch_size\n",
        "\n",
        "  history=model.fit(train, validation_data=valid, validation_steps=validation_steps,\n",
        "            epochs=25, steps_per_epoch=steps_per_epoch,\n",
        "            callbacks=[reduce_lr])\n",
        "\n",
        "  if not os.path.exists(save_model_path):\n",
        "      os.makedirs(save_model_path)\n",
        "\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  # decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  decoder_states_inputs = [decoder_state_input_h]\n",
        "  decoder_outputs, state_h = decoder_gru(\n",
        "      decoder_inputs, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [state_h]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = tf.keras.models.Model(\n",
        "      [decoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  # encoder_model.summary()\n",
        "  # decoder_model.summary()\n",
        "\n",
        "  # saving the models\n",
        "  encoder_model.save(os.path.join(save_model_path, 'encoder_model.h5'))\n",
        "  decoder_model.save_weights(os.path.join(save_model_path, 'decoder_model_weights.h5'))\n",
        "  with open(os.path.join(save_model_path, 'tokenizer' + str(num_decoder_tokens)), 'wb') as file:\n",
        "      joblib.dump(tokenizer, file)\n",
        "  \n",
        "  return history\n",
        "\n",
        "history = train_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBskg4DVZldh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.grid(True)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy vs No. of Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOloDcfGZnkU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "plt.grid(True)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss vs No. of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_yreWGwyxiT"
      },
      "outputs": [],
      "source": [
        "file = open('/content/drive/MyDrive/MP/test_data.txt','w')\n",
        "for id in videoIDs[-100:]:\n",
        "  print(id,file=file)\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8KUbuo4DART"
      },
      "outputs": [],
      "source": [
        "# !unzip 'drive/MyDrive/IT416/model_gru.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnmvaD8JQzCW"
      },
      "outputs": [],
      "source": [
        "# class to perform inference on all test files and save as test_output.txt\n",
        "class Video2Text(object):\n",
        "    ''' Initialize the parameters for the model '''\n",
        "    def __init__(self):\n",
        "        self.latent_dim = 512\n",
        "        self.num_encoder_tokens = 2048\n",
        "        self.num_decoder_tokens = 1500\n",
        "        self.time_steps_encoder = 28\n",
        "        self.time_steps_decoder = None\n",
        "        self.max_probability = -1\n",
        "\n",
        "        # processed data\n",
        "        self.encoder_input_data = []\n",
        "        self.decoder_input_data = []\n",
        "        self.decoder_target_data = []\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # models\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "        self.inf_encoder_model = None\n",
        "        self.inf_decoder_model = None\n",
        "        self.save_model_path = 'model_final'\n",
        "        self.test_path_global = 'global_features'\n",
        "        self.test_path_motion = 'motion_features1'\n",
        "        self.test_path_local = 'local_features1'\n",
        "\n",
        "    def load_inference_models(self):\n",
        "        # load tokenizer\n",
        "        with open(os.path.join(self.save_model_path, 'tokenizer' + str(self.num_decoder_tokens)), 'rb') as file:\n",
        "            self.tokenizer = joblib.load(file)\n",
        "\n",
        "        # inference encoder model\n",
        "        self.inf_encoder_model = tf.keras.models.load_model(os.path.join(self.save_model_path, 'encoder_model.h5'),custom_objects={'attention':attention})\n",
        "\n",
        "        # inference decoder model\n",
        "        decoder_inputs = tf.keras.layers.Input(shape=(None, self.num_decoder_tokens))\n",
        "        decoder_dense = tf.keras.layers.Dense(self.num_decoder_tokens, activation='softmax')\n",
        "        \n",
        "        decoder_gru = tf.keras.layers.GRU(self.latent_dim, return_sequences=True, return_state=True)\n",
        "        decoder_state_input_h = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
        "        # decoder_state_input_c = tf.keras.layers.Input(shape=(self.latent_dim,))\n",
        "        decoder_states_inputs = [decoder_state_input_h]\n",
        "        decoder_outputs, state_h = decoder_gru(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "        decoder_states = [state_h]\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "        self.inf_decoder_model = tf.keras.models.Model(\n",
        "            [decoder_inputs] + decoder_states_inputs,\n",
        "            [decoder_outputs] + decoder_states)\n",
        "\n",
        "        self.inf_decoder_model.load_weights(os.path.join(self.save_model_path, 'decoder_model_weights.h5'))\n",
        "        print('Loaded Inference Model')\n",
        "\n",
        "    \n",
        "    def index_to_word(self):\n",
        "        # inverts word tokenizer\n",
        "        index_to_word = {value: key for key, value in self.tokenizer.word_index.items()}\n",
        "        return index_to_word\n",
        "    \n",
        "    def greedy_search(self, f):\n",
        "        \"\"\"\n",
        "        :param f: the loaded numpy array after creating videos to frames and extracting features\n",
        "        :return: the final sentence which has been predicted greedily\n",
        "        \"\"\"\n",
        "        inv_map = self.index_to_word()\n",
        "        states_value = self.inf_encoder_model.predict(f.reshape(-1, 28, 10240))\n",
        "        states_value = np.array(states_value).reshape((1, 512))\n",
        "        target_seq = np.zeros((1, 1, 1500))\n",
        "        sentence = ''\n",
        "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
        "        for i in range(15):\n",
        "            output_tokens, h = self.inf_decoder_model.predict([target_seq] + [states_value])\n",
        "            states_value = [h]\n",
        "            output_tokens = output_tokens.reshape(self.num_decoder_tokens)\n",
        "            y_hat = np.argmax(output_tokens)\n",
        "            if y_hat == 0:\n",
        "                continue\n",
        "            if inv_map[y_hat] is None:\n",
        "                break\n",
        "            else:\n",
        "                sentence = sentence + inv_map[y_hat] + ' '\n",
        "                target_seq = np.zeros((1, 1, 1500))\n",
        "                target_seq[0, 0, y_hat] = 1\n",
        "        return ' '.join(sentence.split()[:-1])\n",
        "\n",
        "    def decode_sequence2bs(self, input_seq):\n",
        "        states_value = self.inf_encoder_model.predict(input_seq)\n",
        "        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "        target_seq[0, 0, self.tokenizer.word_index['bos']] = 1\n",
        "        self.beam_search(target_seq, states_value,[],[],0)\n",
        "        return decode_seq\n",
        "\n",
        "    def beam_search(self, target_seq, states_value, prob,  path, lens):\n",
        "        global decode_seq\n",
        "        node = 2\n",
        "        # target_seq = np.array(target_seq)\n",
        "        # states_value = np.array(states_value)\n",
        "        states_value = np.array(states_value).reshape((1, 512))\n",
        "        output_tokens, h = self.inf_decoder_model.predict(\n",
        "            [target_seq] + [states_value])\n",
        "        output_tokens = output_tokens.reshape((self.num_decoder_tokens))\n",
        "        sampled_token_index = output_tokens.argsort()[-node:][::-1]\n",
        "        states_value = [h]\n",
        "        for i in range(node):\n",
        "            if sampled_token_index[i] == 0:\n",
        "                sampled_char = ''\n",
        "            else:\n",
        "                sampled_char = list(self.tokenizer.word_index.keys())[list(self.tokenizer.word_index.values()).index(sampled_token_index[i])]\n",
        "            MAX_LEN = 10\n",
        "            if(sampled_char != 'eos' and lens <= MAX_LEN):\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                if(sampled_char == ''):\n",
        "                    p = 1\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                path_new = list(path)\n",
        "                path_new.append(sampled_char)\n",
        "                target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n",
        "                target_seq[0, 0, sampled_token_index[i]] = 1.\n",
        "                self.beam_search(target_seq, states_value, prob_new, path_new, lens+1)\n",
        "            else:\n",
        "                p = output_tokens[sampled_token_index[i]]\n",
        "                prob_new = list(prob)\n",
        "                prob_new.append(p)\n",
        "                p = functools.reduce(operator.mul, prob_new, 1)\n",
        "                if(p > self.max_probability):\n",
        "                    decode_seq = path\n",
        "                    self.max_probability = p\n",
        "\n",
        "    def decoded_sentence_tuning(self, decoded_sentence):\n",
        "        decode_str = []\n",
        "        filter_string = ['bos', 'eos']\n",
        "        unigram = {}\n",
        "        last_string = \"\"\n",
        "        for idx2, c in enumerate(decoded_sentence):\n",
        "            if c in unigram:\n",
        "                unigram[c] += 1\n",
        "            else:\n",
        "                unigram[c] = 1\n",
        "            if(last_string == c and idx2 > 0):\n",
        "                continue\n",
        "            if c in filter_string:\n",
        "                continue\n",
        "            if len(c) > 0:\n",
        "                decode_str.append(c)\n",
        "            if idx2 > 0:\n",
        "                last_string = c\n",
        "        return decode_str\n",
        "\n",
        "    def get_test_data(self, path_global, path_motion, path_local):\n",
        "        X_test = []\n",
        "        X_test_filename = []\n",
        "        with open('/content/drive/MyDrive/MP/test_data.txt') as testing_file:\n",
        "            lines = testing_file.readlines()\n",
        "            for filename in lines:\n",
        "                filename = filename.strip()\n",
        "                f1 = np.load(os.path.join(path_global , filename + '.npy'))\n",
        "                f2 = np.load(os.path.join(path_motion , filename + '.npy'))\n",
        "                f3 = np.load(os.path.join(path_motion , filename + '.npy'))\n",
        "                X_test.append(np.concatenate((f1,f2,f3),axis=1))\n",
        "                X_test_filename.append(filename)\n",
        "            X_test = np.array(X_test)\n",
        "            print('X_test.shape:',X_test.shape)\n",
        "        return X_test, X_test_filename\n",
        "\n",
        "    def test(self):\n",
        "        X_test, X_test_filename = self.get_test_data(os.path.join(self.test_path_global),os.path.join(self.test_path_motion),os.path.join(self.test_path_local))\n",
        "        print(len(X_test), len(X_test_filename))\n",
        "        # generate inference test outputs\n",
        "        with open(os.path.join(self.save_model_path, 'test_output.txt'), 'w') as file:\n",
        "            for idx, x in enumerate(X_test): \n",
        "                file.write(X_test_filename[idx]+',')\n",
        "                # decoded_sentence = self.decode_sequence2bs(x.reshape(-1, 28, 10240))\n",
        "                # decode_str = self.decoded_sentence_tuning(decoded_sentence)\n",
        "                # for d in decode_str:\n",
        "                #     file.write(d + ' ')\n",
        "              \n",
        "                decoded_sentence = self.greedy_search(x.reshape(-1, 28, 10240))\n",
        "                file.write(decoded_sentence)\n",
        "\n",
        "                file.write('\\n')\n",
        "                # re-init max prob\n",
        "                self.max_probability = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhWxaH4YhgWE"
      },
      "outputs": [],
      "source": [
        "c = Video2Text()\n",
        "c.load_inference_models()\n",
        "c.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw2nFY3O5-NJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hjUUMns47jD"
      },
      "outputs": [],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "encoder_inputs_global = tf.keras.layers.Input(shape=(time_steps_encoder, 2048), name=\"encoder_inputs_global\")\n",
        "encoder_inputs_motion = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_motion\")\n",
        "encoder_inputs_local = tf.keras.layers.Input(shape=(time_steps_encoder, 4096), name=\"encoder_inputs_local\")\n",
        "#add concat layer\n",
        "encoder_inputs = tf.keras.layers.Concatenate(axis=2)([encoder_inputs_global, encoder_inputs_motion, encoder_inputs_local])\n",
        "\n",
        "encoder = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='encoder_gru')\n",
        "# _, state_h = encoder(encoder_inputs)\n",
        "\n",
        "encoder_seq_output, state_h = encoder(encoder_inputs)\n",
        "attention_output = attention(return_sequences=False)(encoder_seq_output)\n",
        "# encoder = tf.keras.layers.GRU(latent_dim, return_state=True, return_sequences=True, name='encoder_lstm')\n",
        "# _, state_h_1 = encoder(attention_output)\n",
        "encoder_states = [attention_output]\n",
        "\n",
        "# encoder_states = [state_h]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(time_steps_decoder, num_decoder_tokens), name=\"decoder_inputs\")\n",
        "decoder_gru = tf.keras.layers.GRU(latent_dim, return_sequences=True, return_state=True, name='decoder_gru')\n",
        "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_outputs = tf.keras.layers.Dropout(0.3)(decoder_outputs)\n",
        "decoder_dense = tf.keras.layers.Dense(num_decoder_tokens, activation='sigmoid', name='decoder_sigmoid')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs_global,encoder_inputs_motion, encoder_inputs_local, decoder_inputs], decoder_outputs)\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iaipcqCb_Gn"
      },
      "source": [
        "##Evaluation Metrics\n",
        "\n",
        "Coco eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74OuJJQYQhdI"
      },
      "outputs": [],
      "source": [
        "#Prepare training and validation data\n",
        "df = pd.read_csv('/content/drive/MyDrive/MP/data.csv')\n",
        "test_ids = videoIDs[-100:]\n",
        "\n",
        "references_list=[]\n",
        "ref_dict={'annotations':[]}\n",
        "for i in range(len(test_ids)):\n",
        "  df_temp=df.loc[df['VideoID']==test_ids[i]]\n",
        "  references=[]\n",
        "  for _, row in df_temp.iterrows():\n",
        "    reference = row['Description'].split()\n",
        "    ref_dict['annotations'].append({u'video_id':i,u'caption':row['Description']})\n",
        "    references.append(reference)\n",
        "  references_list.append(references)\n",
        "\n",
        "print(ref_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JdLN_WTQqA3"
      },
      "outputs": [],
      "source": [
        "hypotheses = []\n",
        "count=0\n",
        "hyp_dict={'annotations':[]}\n",
        "with open('/content/model_final/test_output.txt','r') as file:\n",
        "  for line in file:\n",
        "    vid = line.split(',')[0]\n",
        "    hyp = line.split(',')[1]\n",
        "    hyp=hyp.split('\\n')[0]\n",
        "    hypotheses.append(hyp)\n",
        "    hyp_dict['annotations'].append({u'video_id':count,u'caption':hyp})\n",
        "    count=count+1\n",
        "\n",
        "print(hyp_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrDuRedIQqVL"
      },
      "outputs": [],
      "source": [
        "!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPBhAhmuQtXA"
      },
      "outputs": [],
      "source": [
        "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
        "from pycocoevalcap.bleu.bleu import Bleu\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "class COCOEvalCap:\n",
        "    def __init__(self,images,gts,res):\n",
        "        self.evalImgs = []\n",
        "        self.eval = {}\n",
        "        self.imgToEval = {}\n",
        "        self.params = {'video_id': images}\n",
        "        self.gts = gts\n",
        "        self.res = res\n",
        "\n",
        "    def evaluate(self):\n",
        "        imgIds = self.params['video_id']\n",
        "        gts = self.gts\n",
        "        res = self.res\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('tokenization...')\n",
        "        tokenizer = PTBTokenizer()\n",
        "        gts  = tokenizer.tokenize(gts)\n",
        "        res = tokenizer.tokenize(res)\n",
        "\n",
        "        # =================================================\n",
        "        # Set up scorers\n",
        "        # =================================================\n",
        "        print('setting up scorers...')\n",
        "        scorers = [\n",
        "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
        "            (Meteor(),\"METEOR\"),\n",
        "            (Rouge(), \"ROUGE_L\"),\n",
        "            (Cider(), \"CIDEr\")\n",
        "        ]\n",
        "\n",
        "        # =================================================\n",
        "        # Compute scores\n",
        "        # =================================================\n",
        "        eval = {}\n",
        "        for scorer, method in scorers:\n",
        "            print('computing %s score...'%(scorer.method()))\n",
        "            score, scores = scorer.compute_score(gts, res)\n",
        "            if type(method) == list:\n",
        "                for sc, scs, m in zip(score, scores, method):\n",
        "                    self.setEval(sc, m)\n",
        "                    self.setImgToEvalImgs(scs, imgIds, m)\n",
        "                    print(\"%s: %0.3f\"%(m, sc))\n",
        "            else:\n",
        "                self.setEval(score, method)\n",
        "                self.setImgToEvalImgs(scores, imgIds, method)\n",
        "                print(\"%s: %0.3f\"%(method, score))\n",
        "        self.setEvalImgs()\n",
        "\n",
        "    def setEval(self, score, method):\n",
        "        self.eval[method] = score\n",
        "\n",
        "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
        "        for imgId, score in zip(imgIds, scores):\n",
        "            if not imgId in self.imgToEval:\n",
        "                self.imgToEval[imgId] = {}\n",
        "                self.imgToEval[imgId][\"video_id\"] = imgId\n",
        "            self.imgToEval[imgId][method] = score\n",
        "\n",
        "    def setEvalImgs(self):\n",
        "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]\n",
        "\n",
        "def calculate_metrics(rng,datasetGTS,datasetRES):\n",
        "    imgIds = rng\n",
        "    gts = {}\n",
        "    res = {}\n",
        "\n",
        "    imgToAnnsGTS = {ann['video_id']: [] for ann in datasetGTS['annotations']}\n",
        "    for ann in datasetGTS['annotations']:\n",
        "        imgToAnnsGTS[ann['video_id']] += [ann]\n",
        "\n",
        "    imgToAnnsRES = {ann['video_id']: [] for ann in datasetRES['annotations']}\n",
        "    for ann in datasetRES['annotations']:\n",
        "        imgToAnnsRES[ann['video_id']] += [ann]\n",
        "\n",
        "    for imgId in imgIds:\n",
        "        gts[imgId] = imgToAnnsGTS[imgId]\n",
        "        res[imgId] = imgToAnnsRES[imgId]\n",
        "\n",
        "    evalObj = COCOEvalCap(imgIds,gts,res)\n",
        "    evalObj.evaluate()\n",
        "    return evalObj.eval\n",
        "\n",
        "rng = range(100)\n",
        "print(calculate_metrics(rng,ref_dict,hyp_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2CpHy7OSGzc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}